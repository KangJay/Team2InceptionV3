{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "instant-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "# Log in\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "ia = InteractiveLoginAuthentication(tenant_id='72f988bf-86f1-41af-91ab-2d7cd011db47')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "external-castle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "connect_str = 'DefaultEndpointsProtocol=https;AccountName=tinyimagstorage0691ba34b;AccountKey=af3IYOodEqOxfhuZhOj050Skaf16szDbwxzhZa8Uh4T8v0lJSVj8ue6IraKIgBYgkUn8G1zND4DMt5SojBI/sQ==;EndpointSuffix=core.windows.net'\n",
    "# Create the BlobServiceClient object which will be used to create a container client\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "spare-messenger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.storage.blob._blob_service_client.BlobServiceClient at 0x11dd54e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob_service_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "prescription-couple",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\n",
      "If you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\n",
      "Please refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "ws = Workspace.get(name=\"tiny-image-net\",\n",
    "               subscription_id='92c76a2f-0e1c-4216-b65e-abf7a3f34c1e',\n",
    "               resource_group='AzureML_UW_ImageClassification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "continuous-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a named datastore from the current workspace\n",
    "datastore = Datastore.get(ws, datastore_name='tinyimagstorage0691ba34b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "human-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinyimagstorage0691ba34b AzureBlob\n",
      "myblobdatastore AzureBlob\n",
      "workspacefilestore AzureFile\n",
      "workspaceblobstore AzureBlob\n"
     ]
    }
   ],
   "source": [
    "# List all datastores registered in the current workspace\n",
    "datastores = ws.datastores\n",
    "for name, datastore in datastores.items():\n",
    "    print(name, datastore.datastore_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "established-northern",
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "retired-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_tinyimagstorage0691ba34b"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "preceding-decrease",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AzureBlobDatastore' object has no attribute 'mount'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d4865e5c5f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# mount dataset onto the mounted_path of a Linux-based compute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmount_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatastore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmounted_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmount_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AzureBlobDatastore' object has no attribute 'mount'"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "mounted_path = tempfile.mkdtemp()\n",
    "\n",
    "# mount dataset onto the mounted_path of a Linux-based compute\n",
    "mount_context = datastore.mount(mounted_path)\n",
    "\n",
    "mount_context.start()\n",
    "\n",
    "import os\n",
    "print(os.listdir(mounted_path))\n",
    "print (mounted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adverse-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ws.datasets['Tiny ImageNet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-norman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dominant-harbor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.data.file_dataset.FileDataset"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-dominican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "gothic-cement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "czech-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all of the needed library\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "import os\n",
    "\n",
    "#Initiate Google Drive to fetch and save your files and datasets\n",
    "#drive.mount('/content/gdrive')\n",
    "#os.chdir('/content/gdrive/My Drive/Colab Notebooks/')\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "\n",
    "#In the last few years, experts have turned to global average pooling (GAP) layers to minimize overfitting by \n",
    "#reducing the total number of parameters in the model. Similar to max pooling layers, GAP layers are used to reduce\n",
    "#the spatial dimensions of a three-dimensional tensor. However, GAP layers perform a more extreme type of \n",
    "#dimensionality reduction, where a tensor with dimensions h×w×d is reduced in size to have dimensions 1×1×d \n",
    "#GAP layers reduce each h×w feature map to a single number by simply taking the average of all hw values\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(200, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "understanding-organic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 images belonging to 200 classes.\n",
      "Found 10000 validated image filenames belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_img_size=(96, 96) # Can change\n",
    "seed = 0\n",
    "\n",
    "\n",
    "#1, 2, 3, 4 are temporary column names. These aren't needed and are dropped.\n",
    "val_data = pd.read_csv(\"./val/val_annotations.txt\", sep=\"\\t\",\n",
    "                  header=None, names=['file', 'class', '1', '2', '3', '4'])\n",
    "val_data.drop(['1', '2', '3', '4'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#Generate train, validation, and test data\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "target_img_size=(96, 96)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train/',\n",
    "        target_size=target_img_size,\n",
    "        color_mode='rgb',\n",
    "        batch_size=64,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_data_generator = valid_datagen.flow_from_dataframe(val_data,\n",
    "                                directory=\"./val/images\", \n",
    "                                x_col=\"file\", y_col=\"class\",\n",
    "                                target_size=target_img_size, color_mode=\"rgb\",\n",
    "                                class_mode=\"categorical\", batch_size=64,\n",
    "                                shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "center-sapphire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 4.7024WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
      "200/200 [==============================] - 184s 922ms/step - loss: 4.7024 - val_loss: 3.9693\n",
      "Epoch 2/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.7750WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 62s 309ms/step - loss: 3.7750\n",
      "Epoch 3/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.5546WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 66s 328ms/step - loss: 3.5546\n",
      "Epoch 4/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.4540WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 65s 323ms/step - loss: 3.4540\n",
      "Epoch 5/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.3789WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 79s 397ms/step - loss: 3.3789\n",
      "Epoch 6/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.3639WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 71s 357ms/step - loss: 3.3639\n",
      "Epoch 7/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2681WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 75s 377ms/step - loss: 3.2681\n",
      "Epoch 8/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2775WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 82s 409ms/step - loss: 3.2775\n",
      "Epoch 9/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2657WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 110s 548ms/step - loss: 3.2657\n",
      "Epoch 10/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2764WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 72s 362ms/step - loss: 3.2764\n",
      "Epoch 11/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2668WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 92s 459ms/step - loss: 3.2668\n",
      "Epoch 12/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2724WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 75s 373ms/step - loss: 3.2724\n",
      "Epoch 13/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2655WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 69s 344ms/step - loss: 3.2655\n",
      "Epoch 14/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2426WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 72s 360ms/step - loss: 3.2426\n",
      "Epoch 15/15\n",
      "200/200 [==============================] - ETA: 0s - loss: 3.2365WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
      "200/200 [==============================] - 79s 393ms/step - loss: 3.2365\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './models/first_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b1dec8918359>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                     callbacks=[earlystop])\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/first_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m   1978\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0;32m-> 1979\u001b[0;31m                     signatures, options)\n\u001b[0m\u001b[1;32m   1980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m   def save_weights(self,\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m    130\u001b[0m     hdf5_format.save_model_to_hdf5(\n\u001b[0;32m--> 131\u001b[0;31m         model, filepath, overwrite, include_optimizer)\n\u001b[0m\u001b[1;32m    132\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './models/first_model.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 602)"
     ]
    }
   ],
   "source": [
    "from keras import callbacks\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "earlystop = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5,\n",
    "                                    restore_best_weights=True)\n",
    "# earlystop = callbacks.ReduceLROnPlateau(mode=\"min\")\n",
    "\n",
    "model.fit_generator(train_generator, epochs=15, steps_per_epoch=200,\n",
    "                    validation_steps=200,\n",
    "                    validation_data=validation_data_generator, verbose=1,\n",
    "                    callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "premier-luxury",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./inception_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-exercise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-fundamental",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
