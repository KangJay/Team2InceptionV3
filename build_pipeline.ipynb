{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goes Through How to Train Models via AML Pipelines\n",
    "First we need to get our Azure SDK imports, authentication, etc.\n",
    "\n",
    "If you're running this, use your own tenant-id, because AzureML workspaces authenticate using InteractiveLoginAuthentication and obviously it won't work because you're not signed into my Azure account. Can find this by searching for tenant properties under services. \n",
    "\n",
    "1. We get our workspace\n",
    "2. We grab the dataset from our registered dataset list\n",
    "3. Using \"as_mount\" converts this into a DataSetConsumptionConfig object which is what pipelines will use to transfer data inbetween places.\n",
    "\n",
    "The Tiny Imagenet dataset already comes prepped. So we didn't need a data-prep step. Can be an additional step here for other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1621239510411
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core import Workspace #azureml-core of version 1.0.72 or higher is required\n",
    "from azureml.core import Workspace, Dataset, Datastore\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "# If someone else is running this, put your tenant id here... auth is being finnicky\n",
    "interactive_auth = InteractiveLoginAuthentication(tenant_id=\"72f988bf-86f1-41af-91ab-2d7cd011db47\")\n",
    "# get/create experiment\n",
    "ws = Workspace.get(\n",
    "    subscription_id=\"92c76a2f-0e1c-4216-b65e-abf7a3f34c1e\",\n",
    "    resource_group=\"azureml_uw_imageclassification\",\n",
    "    name=\"tiny-image-net\",\n",
    "    auth=interactive_auth\n",
    ")\n",
    "\n",
    "# get dataset (FileDataset object)\n",
    "tiny_imagenet = Dataset.get_by_name(ws, name='Tiny ImageNet')\n",
    "# Convert to DataSetConsumptionConfig object\n",
    "#ds_input = tiny_imagenet.as_named_input(\"tiny_imagenet_dataset\")\n",
    "ds_input = tiny_imagenet.as_mount(path_on_compute=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azureml.data.dataset_consumption_config.DatasetConsumptionConfig"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This import \"OutputFileDatasetConfig\" is what lets us connect output from one script to another. In this case, I'm using it to store our saved models to our default datastore so I can just make sure everything's working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Clusters\n",
    "If there aren't any of the specific clusters we need running in the workspace already, create them. We really only need 1 node. Ignore the max_nodes. This is a WIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found compute target: compute-cluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "compute_name = \"compute-cluster\"\n",
    "vm_size = \"STANDARD_NC12\"\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('Found compute target: ' + compute_name)\n",
    "else:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,  # STANDARD_NC6 is GPU-enabled\n",
    "                                                                vm_priority='lowpriority',\n",
    "                                                                min_nodes=1,\n",
    "                                                                max_nodes=4)\n",
    "    # create the compute target\n",
    "    compute_target = ComputeTarget.create(\n",
    "        ws, compute_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # For a more detailed view of current cluster status, use the 'status' property\n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Training Environment\n",
    "Need to include the dependencies in order to train the model. Using a curated environment will use pre-built Docker images from the Microsoft Container Registry. Conda dependencies and pip dependencies do not like each other..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration, DockerConfiguration, MpiConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core import Environment \n",
    "\n",
    "# Use the 4 nodes we have\n",
    "#distributed_training_config = MpiConfiguration(node_count=4)\n",
    "\n",
    "aml_run_config = RunConfiguration()\n",
    "aml_run_config.target = compute_target\n",
    "\n",
    "USE_CURATED_ENV = False\n",
    "\n",
    "if USE_CURATED_ENV:\n",
    "    # We don't have a curated environment set up\n",
    "    curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
    "    aml_run_config.environment = curated_environment\n",
    "else: \n",
    "    aml_run_config.environment.python.user_managed_dependencies = False\n",
    "    # base docker environment\n",
    "    #aml_run_config.environment.docker = DockerConfiguration(use_docker=True)\n",
    "    aml_run_config.environment.docker.base_image = 'mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04'\n",
    "    \n",
    "    # Add some packages relied on by data prep step\n",
    "    conda_dependencies = CondaDependencies.create(\n",
    "        #conda_packages=['tensorflow-gpu==2.2.0'],\n",
    "        conda_packages=[], \n",
    "        pin_sdk_version=False)\n",
    "    pip_packages=['joblib', 'pandas', 'tensorflow==2.2.0', 'keras', 'pillow','azureml-sdk', 'azureml-dataprep[fuse,pandas, random, math, os, warnings]'] \n",
    "    for pip_package in pip_packages:\n",
    "        conda_dependencies.add_pip_package(pip_package)\n",
    "    aml_run_config.environment.python.conda_dependencies = conda_dependencies\n",
    "aml_run_config.environment.name = \"InceptionV3_TinyImagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "    \"databricks\": {\n",
       "        \"eggLibraries\": [],\n",
       "        \"jarLibraries\": [],\n",
       "        \"mavenLibraries\": [],\n",
       "        \"pypiLibraries\": [],\n",
       "        \"rcranLibraries\": []\n",
       "    },\n",
       "    \"docker\": {\n",
       "        \"arguments\": [],\n",
       "        \"baseDockerfile\": null,\n",
       "        \"baseImage\": \"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\",\n",
       "        \"baseImageRegistry\": {\n",
       "            \"address\": null,\n",
       "            \"password\": null,\n",
       "            \"registryIdentity\": null,\n",
       "            \"username\": null\n",
       "        },\n",
       "        \"enabled\": false,\n",
       "        \"platform\": {\n",
       "            \"architecture\": \"amd64\",\n",
       "            \"os\": \"Linux\"\n",
       "        },\n",
       "        \"sharedVolumes\": true,\n",
       "        \"shmSize\": null\n",
       "    },\n",
       "    \"environmentVariables\": {\n",
       "        \"EXAMPLE_ENV_VAR\": \"EXAMPLE_VALUE\"\n",
       "    },\n",
       "    \"inferencingStackVersion\": null,\n",
       "    \"name\": \"InceptionV3_TinyImagenet\",\n",
       "    \"python\": {\n",
       "        \"baseCondaEnvironment\": null,\n",
       "        \"condaDependencies\": {\n",
       "            \"channels\": [\n",
       "                \"anaconda\",\n",
       "                \"conda-forge\"\n",
       "            ],\n",
       "            \"dependencies\": [\n",
       "                \"python=3.6.2\",\n",
       "                {\n",
       "                    \"pip\": [\n",
       "                        \"azureml-defaults\",\n",
       "                        \"joblib\",\n",
       "                        \"pandas\",\n",
       "                        \"tensorflow==2.2.0\",\n",
       "                        \"keras\",\n",
       "                        \"pillow\",\n",
       "                        \"azureml-sdk\",\n",
       "                        \"azureml-dataprep[fuse,pandas, random, math, os, warnings]\"\n",
       "                    ]\n",
       "                }\n",
       "            ],\n",
       "            \"name\": \"azureml_5b83250c591a11f9fc0c1a448508b3c3\"\n",
       "        },\n",
       "        \"condaDependenciesFile\": null,\n",
       "        \"interpreterPath\": \"python\",\n",
       "        \"userManagedDependencies\": false\n",
       "    },\n",
       "    \"r\": null,\n",
       "    \"spark\": {\n",
       "        \"packages\": [],\n",
       "        \"precachePackages\": true,\n",
       "        \"repositories\": []\n",
       "    },\n",
       "    \"version\": \"2\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aml_run_config\n",
    "# Need to name the environment in order to save it to the workspace\n",
    "aml_run_config.environment.register(workspace=ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Datastore to get output\n",
    "Will be using this default datastore for the outputs of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "def_blob_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"def_blob_store\" object is essentially a reference to our default datastore's blob container. That output config object earlier references this. Think of it as passing along a path to the directory we want to save our files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Pipeline Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "train_source_dir = \"./train\"\n",
    "entry_point = \"train.py\"\n",
    "\n",
    "# Place to save the outputted model. This is converted into a path for the model to save. \n",
    "output = OutputFileDatasetConfig(destination=(\n",
    "            def_blob_store,'./model/run_{run-id}'))\n",
    "# So looking inside our datastore... inside the container name listed in the cell above..., \n",
    "# you should see a directory \"model\" with subdirectories representing each run.\n",
    "\n",
    "# Use the 4 nodes we have\n",
    "#distributed_training_config = MpiConfiguration(node_count=4)\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "                script_name=entry_point,\n",
    "                arguments=[\n",
    "                        \"--data_path\", \"./data\", # Path the data is mounted to. Look in train.py\n",
    "                        \"--steps_per_epoch\", 150, #params\n",
    "                        \"--num_epochs\", 10,\n",
    "                        \"--batch_size\", 64,\n",
    "                        \"--save_dest\", output,  #The output config object from earlier. See train.py\n",
    "                        \"--run_eval\", True # Needs to be implemented still\n",
    "                          ],\n",
    "                inputs=[ds_input],\n",
    "                compute_target=compute_target,\n",
    "                source_directory=train_source_dir,\n",
    "                runconfig=aml_run_config,\n",
    "                allow_reuse=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The interactive login thing I mentioned earlier will show up in the jupyter notebook cell... It'll ask you to go to a specific link and enter a short code. I forgot about this one run and left it alone for it to not work for hours.\n",
    "\n",
    "If cold-running this, it'll need to download the docker image for ubuntu & cuda. This may take ~10-13 minutes. Otherwise, it should get to that authentication prompt within a few minutes. \n",
    "\n",
    "Also sidenote: The jupyter cell output pulls from a log.txt file that \"Experiments\" uses. Sometimes this lags/freezes... If this happens, go to the experiments tab, click on the **display name** for the run.\\\n",
    "Then, under \"Graph\", click on the train.py box, azureml-logs > 70_driver_log.txt. The prompt should be there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WAS FOR TESTING. DON'T USE THIS ONE\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "def_blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "train_source_dir = \"./train\"\n",
    "entry_point = \"test.py\"\n",
    "\n",
    "# Place to save the outputted model. This is converted into a path for the model to save. \n",
    "output = OutputFileDatasetConfig(destination=(\n",
    "            def_blob_store,'outputs/run_{run-id}'))\n",
    "# So looking inside our datastore... inside the container name listed in the cell above..., \n",
    "# you should see a directory \"model\" with subdirectories representing each run.\n",
    "\n",
    "# Use the 4 nodes we have\n",
    "#distributed_training_config = MpiConfiguration(node_count=4)\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "                script_name=entry_point,\n",
    "                arguments=[\n",
    "                        \"--data_path\", \"./data\", # Path the data is mounted to. Look in train.py\n",
    "                        \"--steps_per_epoch\", 10, #params\n",
    "                        \"--num_epochs\", 1,\n",
    "                        #\"--batch_size\", 64,\n",
    "                        \"--save_dest\", output_data,  #The output config object from earlier. See train.py\n",
    "                        #\"--run_eval\", True\n",
    "                          ],\n",
    "                inputs=[ds_input],\n",
    "                outputs=[output_data],\n",
    "                compute_target=compute_target,\n",
    "                source_directory=train_source_dir,\n",
    "                runconfig=aml_run_config,\n",
    "                allow_reuse=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(workspace=ws, steps=[train_step])\n",
    "print(\"Pipeline made. Submitting run...\")\n",
    "exp = Experiment(ws, \"Training_InceptionV3\")\n",
    "pipeline_run = exp.submit(pipeline)\n",
    "#run = pipeline_run.start_logging(outputs=None, snapshot_directory=None)\n",
    "print(\"Submitted. Waiting for completion...\")\n",
    "pipeline_run.wait_for_completion()\n",
    "print(\"Completed! Experiment should appear under the Experiments page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(output_data.datastore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Saved\n",
    "The model's been saved to the default datastore associated with the account. We can now load it into our workspace, save a local copy, register the model, etc.\n",
    "\n",
    "We tried utilizing the built in work-flow AzureML has set up by saving directly to outputs or logs directory but some issues have come up that we are not entirely sure why.\n",
    "\n",
    "Saving to those directories would save the files within a directory named \"outputs\" of each step and not the experiment. This meant we couldn't get the files directly after training from the PipelineRun object. Did find another work-around that still has some merits. Still looking into why it's not saving to the output and logs folder for the experiment.\n",
    "\n",
    "We can get the reference to the saved model's location through a couple steps. Previously, we specified an output file path to save the model, we can extract it using the same datastore object.\n",
    "\n",
    "**def_blob_store.download()**. We need to provide the path to save the file to, and the prefix of the path on the datastore.\n",
    "\n",
    "We specified to save the models to: model/run_{run_id}\n",
    "\n",
    "We can get the run id from the pipelinerun object we used to submit the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in pipeline_run.get_steps():\n",
    "    run_id = step.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had to get the id of the step. Every resource we've looked into said saving to **outputs** or **logs** would save them to the experiment output file for persistence - those two specifically were special directory names. \n",
    "\n",
    "E.g. saving an object named \"example\" as example.txt by doing joblib.dump(example, \"objects/example.txt\") in train.py would save it in the output of the step, not the experiment. This download work around is temporary until we can figure out why this portion is not working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def_blob_store.download(target_path=\"./run_output\",\n",
    "                        prefix=f\"model/run_{run_id}\", \n",
    "                        show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the model downloaded, we can register the model, save a local copy, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./run_output/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls ./run_output/model/run_b9d700aa-5b98-4ef2-84f0-851393f692cd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see the saved files there. This solution is not as automated as we want it to be however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up\n",
    "\n",
    "Clean up downloaded model if we're done using it. Run cell if you don't need the model files anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./run_output/"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
